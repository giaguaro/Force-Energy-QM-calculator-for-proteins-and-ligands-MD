{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4c1889",
   "metadata": {},
   "source": [
    "# Using Nequip and Spice dataset to create energy/force calculator for molecular dynmaics applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5fb04",
   "metadata": {},
   "source": [
    "### 1) Download SPICE dataset for atomic potentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14276d9",
   "metadata": {},
   "source": [
    "#### This dataset was computed at the Ï‰B97M-D3BJ/def2-TZVPPD level of theory. It includes 15 elements (H, Li, C, N, O, F, Na, Mg, P, S, Cl, K, Ca, Br, I) and a wide range of chemical groups.  It includes both low and high energy conformations. Categories of molecules include: \n",
    "\n",
    "- Dipeptides: These provide comprehensive sampling of the covalent interactions found in proteins. \n",
    "- Solvated amino acids: These provide sampling of protein-water and water-water interactions. \n",
    "- PubChem molecules: These sample a very wide variety of drug-like small molecules. \n",
    "- Monomer and dimer structures from DES370K: These provide sampling of a wide variety of non-covalent interactions. \n",
    "- Ion pairs: These provide further sampling of Coulomb interactions over a range of distances.\n",
    "\n",
    "\n",
    "\n",
    "Reference: Peter Eastman, Pavan Kumar Behara, David L. Dotson, Raimondas Galvelis, John E. Herr, Josh T. Horton, Yuezhi Mao, John D. Chodera, Benjamin P. Pritchard, Yuanqing Wang, Gianni De Fabritiis, and Thomas E. Markland. \"SPICE, A Dataset of Drug-like Molecules and Peptides for Training Machine Learning Potentials.\" https://doi.org/10.48550/arXiv.2209.10702 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cfc7c",
   "metadata": {},
   "source": [
    "From a command line run the following: \n",
    "    \n",
    "a) conda install necessary libraries\n",
    "`conda install -c conda-forge qcportal pyyaml h5py rdkit numpy`\n",
    "\n",
    "b) clone the SPICE repository \n",
    "`git clone https://github.com/openmm/spice-dataset.git`\n",
    "\n",
    "c) navigate to the downloader/ directory\n",
    "\n",
    "d) configure the config file to include only: <b>'DFT TOTAL ENERGY'</b> and <b>'DFT TOTAL GRADIENT'</b>\n",
    "\n",
    "d) run the downloader \n",
    "`python downloader.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945712d6",
   "metadata": {},
   "source": [
    "### 2) Convert the HDF5 file into an extended xyz dataset for Nequip training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import ase, ase.io\n",
    "import collections\n",
    "\n",
    "# path to the SPICE hdf5 file - MODIFY HERE\n",
    "filename = 'SPICE.h5'\n",
    "data = h5py.File(filename, 'r')\n",
    "\n",
    "dsets = [\"atomic_numbers\", \"conformations\",\"dft_total_energy\",\"dft_total_gradient\"]\n",
    "\n",
    "all_atomic_numbers = list()\n",
    "atomic_numbers_count = collections.defaultdict(int)\n",
    "test_indices = collections.defaultdict(list)\n",
    "\n",
    "# i am splitting here to give represenative molecules (atomic numbers) for each dataset\n",
    "train_filename = \"SPICE_train.extxyz\"\n",
    "test_filename = \"SPICE_test.extxyz\"\n",
    "\n",
    "for idx, group in enumerate(data) :\n",
    "    print(group)\n",
    "    for idx in range(data[group]['conformations'].shape[0]): \n",
    "        fields = {}\n",
    "        dic = {}\n",
    "        for dset in dsets:\n",
    "            if dset == 'conformations':\n",
    "                pos = list(map(tuple, np.array(data[group]['conformations'][idx])))\n",
    "                dic['pos'] = np.array(data[group]['conformations'][idx])\n",
    "                n_nodes = data[group]['conformations'][idx].shape[0]\n",
    "            elif dset == 'atomic_numbers':\n",
    "                atomic_numbers=torch.tensor(np.array((data[group]['atomic_numbers']))).view(-1)\n",
    "                dic['species'] = np.array(data[group]['atomic_numbers'])\n",
    "                an_values = list(np.array(data[group]['atomic_numbers']))\n",
    "                all_atomic_numbers.extend(an_values)\n",
    "                all_atomic_numbers=list(set(all_atomic_numbers))\n",
    "            elif dset == 'dft_total_gradient':\n",
    "                fields[\"forces\"] = np.array(data[group]['dft_total_gradient'][idx])\n",
    "                dic['forces'] = np.array(data[group]['dft_total_gradient'][idx])\n",
    "            elif dset == 'dft_total_energy':\n",
    "                fields[\"energy\"] = np.array(data[group]['dft_total_energy'][idx])\n",
    "                dic['energy'] = np.array(data[group]['dft_total_energy'][idx])\n",
    "        \n",
    "        if len(dic) == 4:\n",
    "            if len(dic['species']) != n_nodes: \n",
    "                break\n",
    "            else:\n",
    "                mol = ase.Atoms(numbers=atomic_numbers, positions=pos)\n",
    "                mol.calc = SinglePointCalculator(mol, **fields)\n",
    "\n",
    "                for an in dic['species']:\n",
    "                    atomic_numbers_count[an] += 1\n",
    "                    if np.random.rand() < 0.1:  # 10% chance for testing\n",
    "                        ase.io.write(\n",
    "                            test_filename,\n",
    "                            mol,\n",
    "                            format=\"extxyz\",\n",
    "                            append=True)\n",
    "                        test_indices[an].append(atomic_numbers_count[an])\n",
    "                    else:\n",
    "                        ase.io.write(\n",
    "                            train_filename,\n",
    "                            mol,\n",
    "                            format=\"extxyz\",\n",
    "                            append=True)\n",
    "        else:\n",
    "            continue\n",
    "                       \n",
    "# open a file, where you want to store the data\n",
    "file = open('union_atomic_numbers_spice', 'wb')\n",
    "\n",
    "\n",
    "pickle.dump(all_atomic_numbers, file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "# concatenate and let Nequip take validation from the bottom 10%\n",
    "filenames = [train_filename, test_filename]\n",
    "with open('SPICE_all.extxyz', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d2479",
   "metadata": {},
   "source": [
    "### 3) Run Nequip training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc17e6",
   "metadata": {},
   "source": [
    "I have changed some things in the Nequip code to handle inconsistent formatting from the SPICE dataset. Do not clone the Nequip respo. Instead, use the code provided here to install from source. Follow these steps from the command line:\n",
    "\n",
    "a) Navigate to the provided Nequip directory (nequip) in this project.\n",
    "\n",
    "b) Run `pip install .`\n",
    "\n",
    "c) Next, navigate to SPICE_TRAINING\n",
    "\n",
    "d) Modify <b>run_train.sh</b> according to the Scheduler present on your system (or lack thereof).\n",
    "\n",
    "e) Run `sbatch run_training.sh` or `sh run_training.sh` (depending on your scheduler settings). \n",
    "\n",
    "##### NOTE: I have a pre-defined spice_config.yaml file for our purpose here\n",
    "\n",
    "\n",
    "References:\n",
    "Batzner, S., Musaelian, A., Sun, L. et al. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nat Commun 13, 2453 (2022). https://doi.org/10.1038/s41467-022-29939-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39b811",
   "metadata": {},
   "source": [
    "### 3) Run inferrence with deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstrative example. Modify according to need\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from nequip.utils import Config\n",
    "from nequip.data import AtomicData, AtomicDataDict\n",
    "\n",
    "#load data\n",
    "config = Config.from_file('configs/minimal.yaml')\n",
    "mydata = np.load('benchmark_data/aspirin_ccsd-test.npz')\n",
    "pos = torch.tensor(mydata['R'][-1])\n",
    "labeled_forces = torch.tensor(mydata['F'][-1])\n",
    "z = mydata['z']\n",
    "ATOMIC_NUMBERS_KEY = torch.Tensor(torch.from_numpy(z.astype(np.float32))).to(torch.int64)\n",
    "ATOM_TYPE_KEY = torch.zeros_like(ATOMIC_NUMBERS_KEY)\n",
    "type_unique = torch.unique(ATOMIC_NUMBERS_KEY)\n",
    "for index, a_type in enumerate(type_unique): ATOM_TYPE_KEY[(ATOMIC_NUMBERS_KEY == a_type).nonzero()] = index\n",
    "data = AtomicData.from_points(pos=pos, r_max=config['r_max'],\n",
    "**{AtomicDataDict.ATOMIC_NUMBERS_KEY: ATOMIC_NUMBERS_KEY,\n",
    "AtomicDataDict.ATOM_TYPE_KEY: ATOM_TYPE_KEY})\n",
    "datadict = AtomicData.to_AtomicDataDict(data)\n",
    "#load model\n",
    "mymodel = torch.load(\"./results/aspirin/aspirin_model.pth\")\n",
    "mymodel.eval()\n",
    "pred = mymodel(datadict)\n",
    "print(pred['total_energy'])\n",
    "print(pred['forces'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
